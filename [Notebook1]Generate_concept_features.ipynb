{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d3f381",
   "metadata": {},
   "source": [
    "# 1 Prepare the Dataset\n",
    "\n",
    "Using the **BloodMNIST** dataset as an example.\n",
    "\n",
    "Prepare the dataset and wrap the data into DataLoaders for batch feature extraction.\n",
    "\n",
    "Prepare the following variables:\n",
    "1. `label_list`: List of label texts, e.g. for BloodMNIST: `['basophil', 'eosinophil', ...]`\n",
    "2. `label_prompt_template_list`: List of prompt templates for labels. Example:\n",
    "    ```python\n",
    "    label_prompt_template_list = [\n",
    "        \"a microscopic image of a {} cell\",\n",
    "        \"a peripheral blood smear image of a {}\",\n",
    "        \"a bloodcell of {}\"\n",
    "    ]\n",
    "    ```\n",
    "3. `concept_list`: List of concept texts (designed by experts), e.g. for BloodMNIST: `['Segmented nucleus','Band nucleus (band form)','Reniform / indented nucleus','Round nucleus', ...]`\n",
    "4. `concept_prompt_template_list`: List of prompt templates for concepts. Example:\n",
    "    ```python\n",
    "    concept_prompt_template_list = [\n",
    "        \"a cell photo with sign of {}\",\n",
    "        \"a photo of a cell with {}\",\n",
    "        \"a cell image indicating {}\",\n",
    "    ]\n",
    "    ```\n",
    "5. `train_loader`, `test_loader`, `valid_loader`: DataLoaders (torch.utils.data.DataLoader) for training, test, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71a753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import medmnist\n",
    "from medmnist import BloodMNIST, INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee96c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select BloodMNIST dataset information\n",
    "data_flag = 'bloodmnist'\n",
    "info = INFO[data_flag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c2c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basophil',\n",
       " 'eosinophil',\n",
       " 'erythroblast',\n",
       " 'immature granulocytes',\n",
       " 'lymphocyte',\n",
       " 'monocyte',\n",
       " 'neutrophil',\n",
       " 'platelet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label mapping dictionary (label_map)\n",
    "label_map = {int(k): v for k, v in info['label'].items()}  # e.g. 0:'basophil', 1:'eosinophil', ...\n",
    "label_map[3] = 'immature granulocytes'\n",
    "\n",
    "# Create list of label texts\n",
    "label_list = [label_map[idx] for idx in sorted(label_map.keys())]\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c094b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 15 cellular morphological features as concepts (manually chosen)\n",
    "cell_features = [\n",
    "    {\"en\": \"Segmented nucleus\", \"zh\": \"分叶细胞核\"},\n",
    "    {\"en\": \"Band nucleus (band form)\", \"zh\": \"带状细胞核（未分叶）\"},\n",
    "    {\"en\": \"Reniform / indented nucleus\", \"zh\": \"肾形/凹陷细胞核\"},\n",
    "    {\"en\": \"Round nucleus\", \"zh\": \"圆形细胞核\"},\n",
    "    {\"en\": \"Fine azurophilic granules\", \"zh\": \"细嗜天青颗粒\"},\n",
    "    {\"en\": \"Eosinophilic granules\", \"zh\": \"嗜酸性颗粒（橙红）\"},\n",
    "    {\"en\": \"Basophilic granules\", \"zh\": \"嗜碱性颗粒（深紫粗颗粒）\"},\n",
    "    {\"en\": \"Basophilic cytoplasm\", \"zh\": \"嗜碱性胞质\"},\n",
    "    {\"en\": \"Cytoplasmic vacuoles\", \"zh\": \"胞质空泡\"},\n",
    "    {\"en\": \"High nuclear-to-cytoplasmic ratio\", \"zh\": \"高核浆比\"},\n",
    "    {\"en\": \"Pale cytoplasm\", \"zh\": \"淡染胞质\"},\n",
    "    {\"en\": \"Nucleated erythrocyte (erythroblast)\", \"zh\": \"有核红细胞（幼红细胞）\"},\n",
    "    {\"en\": \"Platelet fragments / clumps\", \"zh\": \"血小板碎片/成团\"},\n",
    "    {\"en\": \"Stain precipitate (artifact)\", \"zh\": \"染色沉淀（伪影）\"},\n",
    "    {\"en\": \"Overlapping cell clumps (artifact)\", \"zh\": \"细胞重叠/成团（伪影）\"},\n",
    "]\n",
    "\n",
    "# Generate concept text list\n",
    "concept_list = [cell_feature[\"en\"] for cell_feature in cell_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare prompt templates\n",
    "concept_prompt_template_list = [\n",
    "    \"a cell photo with sign of {}\",\n",
    "    \"a photo of a cell with {}\",\n",
    "    \"a cell image indicating {}\",\n",
    "    \"an image of a cell showing {}\",\n",
    "    \"blood cell with {}\",\n",
    "    \"a blood cell photo with sign of {}\",\n",
    "    \"a photo of a blood cell with {}\",\n",
    "    \"a blood cell image indicating {}\",\n",
    "    \"an image of blood cell showing {}\",\n",
    "]\n",
    "\n",
    "label_prompt_template_list = [\n",
    "    \"a microscopic image of a {} cell\",\n",
    "    \"a peripheral blood smear image of a {}\",\n",
    "    \"a bloodcell of {}\",\n",
    "]\n",
    "\n",
    "DB_METADATA = {\n",
    "    \"label_texts\": label_list,\n",
    "    \"prompt_temp_for_labels\": label_prompt_template_list,\n",
    "    \"concept_texts\": concept_list,\n",
    "    \"prompt_temp_for_concepts\": concept_prompt_template_list,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\chenk\\.medmnist\\bloodmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\chenk\\.medmnist\\bloodmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\chenk\\.medmnist\\bloodmnist_224.npz\n",
      "num_train: 11959 sample shape: (3, 224, 224)\n",
      "num_test: 3421 sample shape: (3, 224, 224)\n",
      "num_valid: 1712 sample shape: (3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Acquire BloodMNIST subclass via DataClass\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "common_tf = transforms.Compose([\n",
    "    # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "    # to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "ds_train = DataClass(split='train', size=224, as_rgb=True, mmap_mode='r', transform=common_tf, download=True)\n",
    "ds_test  = DataClass(split='test',  size=224, as_rgb=True, mmap_mode='r', transform=common_tf, download=True)\n",
    "ds_valid = DataClass(split='val',   size=224, as_rgb=True, mmap_mode='r', transform=common_tf, download=True)\n",
    "\n",
    "print(\"num_train:\", len(ds_train), \"sample shape:\", np.array(ds_train[0][0]).shape)  # (224, 224, 3)\n",
    "print(\"num_test:\", len(ds_test), \"sample shape:\", np.array(ds_test[0][0]).shape)    # (224, 224, 3)\n",
    "print(\"num_valid:\", len(ds_valid), \"sample shape:\", np.array(ds_valid[0][0]).shape) # (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(ds_train, batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(ds_test, batch_size=64, shuffle=False)\n",
    "valid_loader = DataLoader(ds_valid, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b649e",
   "metadata": {},
   "source": [
    "# 2 Load the CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52db85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\chenk\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "{'type': 'user', 'id': '6693342048f8a55b7eba3d8e', 'name': 'takedachia', 'fullname': 'Jack Chan', 'email': 'chenkai1989129@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/df59d16febde1a73fc98bf70f5476ba2.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'ConceptCLIP', 'role': 'read', 'createdAt': '2025-08-20T17:55:27.543Z'}}}\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model from HuggingFace\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from huggingface_hub import login, whoami\n",
    "login(token=\"<YOUR_HF_TOKEN>\")  # replace <YOUR_HF_TOKEN> with your actual token\n",
    "print(whoami())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37b1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chenk\\.conda\\envs\\pyg_monai\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\chenk\\.conda\\envs\\pyg_monai\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    }
   ],
   "source": [
    "# Load model and processor (first time will download to local cache)\n",
    "model = AutoModel.from_pretrained('JerrryNie/ConceptCLIP', trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained('JerrryNie/ConceptCLIP', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU if available (else CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa82df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust processor image preprocessing settings\n",
    "# BloodMNIST DataLoader already yields float tensors in [0,1], so we disable rescale and enable normalize\n",
    "processor.image_processor.do_rescale = False\n",
    "processor.image_processor.do_normalize = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207c98d",
   "metadata": {},
   "source": [
    "# 3 Use Text and Image Encoders to Infer, Extract, and Store Features\n",
    "\n",
    "## (1) Deploy H5 database for storing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py, hashlib, json, time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "OUT_PATH = \"./conceptclip_features.h5\"\n",
    "D       = 1152\n",
    "T_img   = 729\n",
    "DTYPE_DYNAMIC = \"float16\"   # Dynamic parts (image_*) can be stored in lower precision to save space\n",
    "DTYPE_TEXT    = \"float32\"   # Text features are recommended to remain in higher precision\n",
    "DTYPE_STR     = h5py.string_dtype(encoding=\"utf-8\")\n",
    "\n",
    "def _serialize_for_attr(value):\n",
    "    \"\"\"Serialize a value for storing as an HDF5 file attribute.\"\"\"\n",
    "    return json.dumps(value, ensure_ascii=False)\n",
    "\n",
    "def _read_json_attr(f: h5py.File, key: str, default=None):\n",
    "    \"\"\"Read a JSON-formatted attribute from the root of the H5 file and parse it.\"\"\"\n",
    "    if key not in f.attrs:\n",
    "        return default\n",
    "    raw = f.attrs[key]\n",
    "    if isinstance(raw, bytes):\n",
    "        raw = raw.decode(\"utf-8\")\n",
    "    if isinstance(raw, str):\n",
    "        try:\n",
    "            return json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            return raw\n",
    "    return raw\n",
    "\n",
    "def _hash_texts(texts: Sequence[str]) -> str:\n",
    "    \"\"\"Compute a SHA256 hash for a list of texts.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    for t in texts:\n",
    "        h.update(t.encode(\"utf-8\")); h.update(b\"\\0\")\n",
    "    return h.hexdigest()\n",
    "\n",
    "def init_file(path, d=D, t_img=T_img, metadata: Optional[dict] = None):\n",
    "    \"\"\"Initialize HDF5 file structure if missing; open in append mode.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        with h5py.File(path, \"w\") as f:\n",
    "            # Create main datasets\n",
    "            f.create_dataset(\"image_features\",\n",
    "                shape=(0, d), maxshape=(None, d),\n",
    "                chunks=(64, d), dtype=DTYPE_DYNAMIC, compression=\"lzf\")\n",
    "            f.create_dataset(\"image_token_features\",\n",
    "                shape=(0, t_img, d), maxshape=(None, t_img, d),\n",
    "                chunks=(4, t_img, d), dtype=DTYPE_DYNAMIC, compression=\"lzf\")\n",
    "            f.create_dataset(\"ids\",\n",
    "                shape=(0,), maxshape=(None,),\n",
    "                chunks=(4096,), dtype=\"int64\", compression=\"lzf\")\n",
    "            f.create_dataset(\"labels\",\n",
    "                shape=(0,), maxshape=(None,),\n",
    "                chunks=(4096,), dtype=\"int64\", compression=\"lzf\")\n",
    "            f.create_dataset(\"split\",\n",
    "                shape=(0,), maxshape=(None,),\n",
    "                chunks=(4096,), dtype=DTYPE_STR, compression=\"lzf\")\n",
    "            \n",
    "            # Create prompt-templates group\n",
    "            f.create_group(\"templates\")\n",
    "            \n",
    "            # Add global attributes (model parameters)\n",
    "            f.attrs[\"D\"] = d\n",
    "            f.attrs[\"T_img\"] = t_img\n",
    "            f.attrs[\"created_at\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "            f.attrs[\"version\"] = \"1.0\"\n",
    "            \n",
    "    f = h5py.File(path, \"a\")\n",
    "    current_len = f[\"image_features\"].shape[0] if \"image_features\" in f else 0\n",
    "    if \"image_token_features\" not in f:\n",
    "        f.create_dataset(\"image_token_features\",\n",
    "            shape=(current_len, t_img, d), maxshape=(None, t_img, d),\n",
    "            chunks=(4, t_img, d), dtype=DTYPE_DYNAMIC, compression=\"lzf\")\n",
    "    if \"ids\" not in f:\n",
    "        f.create_dataset(\"ids\",\n",
    "            shape=(current_len,), maxshape=(None,),\n",
    "            chunks=(4096,), dtype=\"int64\", compression=\"lzf\")\n",
    "    if \"labels\" not in f:\n",
    "        ds_labels = f.create_dataset(\"labels\",\n",
    "            shape=(current_len,), maxshape=(None,),\n",
    "            chunks=(4096,), dtype=\"int64\", compression=\"lzf\")\n",
    "        if current_len > 0:\n",
    "            ds_labels[:] = np.full((current_len,), -1, dtype=\"int64\")\n",
    "    if \"split\" not in f:\n",
    "        ds_split = f.create_dataset(\"split\",\n",
    "            shape=(current_len,), maxshape=(None,),\n",
    "            chunks=(4096,), dtype=DTYPE_STR, compression=\"lzf\")\n",
    "        if current_len > 0:\n",
    "            ds_split[:] = np.array([\"unspecified\"] * current_len, dtype=object)\n",
    "    if \"templates\" not in f:\n",
    "        f.create_group(\"templates\")\n",
    "    if metadata:\n",
    "        for key, value in metadata.items():\n",
    "            try:\n",
    "                f.attrs[key] = _serialize_for_attr(value)\n",
    "            except TypeError:\n",
    "                f.attrs[key] = _serialize_for_attr(str(value))\n",
    "    return f\n",
    "\n",
    "def _to_np(x, dtype):\n",
    "    \"\"\"Convert tensor to numpy array with specified dtype.\"\"\"\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    return x.astype(dtype, copy=False)\n",
    "\n",
    "def append_batch(f: h5py.File,\n",
    "                 image_feats: torch.Tensor,            # [B, D]\n",
    "                 image_token_feats: torch.Tensor,      # [B, T_img, D]\n",
    "                 ids: np.ndarray,                     # [B] int64\n",
    "                 split_names: Optional[Sequence[str]] = None,\n",
    "                 labels: Optional[np.ndarray] = None):\n",
    "    \"\"\"Append a batch of data to the HDF5 file.\"\"\"\n",
    "    ds_img = f[\"image_features\"]\n",
    "    ds_tok = f[\"image_token_features\"]\n",
    "    ds_ids = f[\"ids\"]\n",
    "    ds_labels = f[\"labels\"]\n",
    "    ds_split = f[\"split\"]\n",
    "    B = image_feats.shape[0]\n",
    "\n",
    "    n0 = ds_img.shape[0]\n",
    "    ds_img.resize(n0 + B, axis=0)\n",
    "    ds_tok.resize(n0 + B, axis=0)\n",
    "    ds_ids.resize(n0 + B, axis=0)\n",
    "    ds_labels.resize(n0 + B, axis=0)\n",
    "    ds_split.resize(n0 + B, axis=0)\n",
    "\n",
    "    ds_img[n0:n0+B, :]      = _to_np(image_feats, DTYPE_DYNAMIC)\n",
    "    ds_tok[n0:n0+B, :, :]   = _to_np(image_token_feats, DTYPE_DYNAMIC)\n",
    "    ds_ids[n0:n0+B]         = ids.astype(\"int64\", copy=False)\n",
    "\n",
    "    if labels is None:\n",
    "        labels_arr = np.full((B,), -1, dtype=\"int64\")\n",
    "    else:\n",
    "        labels_arr = np.asarray(labels, dtype=\"int64\").reshape(-1)\n",
    "        if labels_arr.shape[0] != B:\n",
    "            raise ValueError(f\"Label count {labels_arr.shape[0]} does not match batch size {B}.\")\n",
    "    ds_labels[n0:n0+B] = labels_arr\n",
    "\n",
    "    if split_names is None:\n",
    "        split_arr = np.array([\"unspecified\"] * B, dtype=object)\n",
    "    else:\n",
    "        split_arr = np.asarray(list(split_names), dtype=object).reshape(-1)\n",
    "        if split_arr.shape[0] != B:\n",
    "            raise ValueError(f\"Split count {split_arr.shape[0]} does not match batch size {B}.\")\n",
    "    ds_split[n0:n0+B] = split_arr\n",
    "\n",
    "    f.flush()\n",
    "\n",
    "def write_template(f: h5py.File,\n",
    "                   template_id: str,\n",
    "                   texts: Sequence[str],\n",
    "                   text_features: torch.Tensor,            # [K, D]\n",
    "                   text_token_features: Optional[torch.Tensor]=None # [K, T_txt, D]\n",
    "                   ):\n",
    "    \"\"\"Write text templates and their features.\"\"\"\n",
    "    g_root = f[\"templates\"]\n",
    "    if template_id in g_root:\n",
    "        del g_root[template_id]             # Overwrite existing (change logic if verification needed)\n",
    "    g = g_root.create_group(template_id)\n",
    "\n",
    "    # Main data\n",
    "    tf  = _to_np(text_features, DTYPE_TEXT)         # [K, D]\n",
    "    g.create_dataset(\"text_features\", data=tf, compression=\"lzf\")\n",
    "\n",
    "    if text_token_features is not None:\n",
    "        ttf = _to_np(text_token_features, DTYPE_TEXT)     # [K, T_txt, D]\n",
    "        g.create_dataset(\"text_token_features\", data=ttf, compression=\"lzf\")\n",
    "        T_txt = ttf.shape[1]\n",
    "    else:\n",
    "        T_txt = -1\n",
    "\n",
    "    # Save original prompts (variable-length UTF-8 strings)\n",
    "    dt_str = h5py.string_dtype(encoding='utf-8')\n",
    "    ds_txt = g.create_dataset(\"texts\", shape=(len(texts),), dtype=dt_str, compression=\"lzf\")\n",
    "    ds_txt[:] = np.array(list(texts), dtype=object)\n",
    "\n",
    "    # Metadata\n",
    "    g.attrs[\"K\"]          = tf.shape[0]\n",
    "    g.attrs[\"D\"]          = tf.shape[1]\n",
    "    g.attrs[\"T_txt\"]      = int(T_txt)\n",
    "    g.attrs[\"texts_hash\"] = _hash_texts(texts)\n",
    "    g.attrs[\"created_at\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "    f.flush()\n",
    "\n",
    "def save_model_params(f: h5py.File, \n",
    "                      logit_scale: float = None,\n",
    "                      logit_bias: float = None,\n",
    "                      concept_logit_scale: float = None,\n",
    "                      concept_logit_bias: float = None):\n",
    "    \"\"\"Persist key model parameters to root attributes of the HDF5 file.\"\"\"\n",
    "    if logit_scale is not None:\n",
    "        f.attrs[\"logit_scale\"] = float(logit_scale)\n",
    "    if logit_bias is not None:\n",
    "        f.attrs[\"logit_bias\"] = float(logit_bias)\n",
    "    if concept_logit_scale is not None:\n",
    "        f.attrs[\"concept_logit_scale\"] = float(concept_logit_scale)\n",
    "    if concept_logit_bias is not None:\n",
    "        f.attrs[\"concept_logit_bias\"] = float(concept_logit_bias)\n",
    "    f.flush()\n",
    "\n",
    "def load_model_params(f: h5py.File) -> dict:\n",
    "    \"\"\"Load model parameters from file attributes.\"\"\"\n",
    "    params = {}\n",
    "    for key in [\"logit_scale\", \"logit_bias\", \"concept_logit_scale\", \"concept_logit_bias\"]:\n",
    "        if key in f.attrs:\n",
    "            params[key] = f.attrs[key]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5baaee",
   "metadata": {},
   "source": [
    "## (2) Compute and save concept text features to H5 (using the Text Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aea84e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored templates:\n",
      "  - concept_prompts_t01: 15 prompts\n",
      "  - concept_prompts_t02: 15 prompts\n",
      "  - concept_prompts_t03: 15 prompts\n",
      "  - concept_prompts_t04: 15 prompts\n",
      "  - concept_prompts_t05: 15 prompts\n",
      "  - concept_prompts_t06: 15 prompts\n",
      "  - concept_prompts_t07: 15 prompts\n",
      "  - concept_prompts_t08: 15 prompts\n",
      "  - concept_prompts_t09: 15 prompts\n",
      "  - label_prompts_t01: 8 prompts\n",
      "  - label_prompts_t02: 8 prompts\n",
      "  - label_prompts_t03: 8 prompts\n"
     ]
    }
   ],
   "source": [
    "def _maybe_to_scalar(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.detach()\n",
    "        if x.numel() == 1:\n",
    "            return float(x.item())\n",
    "        try:\n",
    "            return float(x.squeeze().item())\n",
    "        except Exception:\n",
    "            return None\n",
    "    if hasattr(x, \"item\"):\n",
    "        try:\n",
    "            return float(x.item())\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "stored_templates = []\n",
    "with init_file(OUT_PATH, metadata=DB_METADATA) as f:\n",
    "    concept_texts_in_file = _read_json_attr(f, \"concept_texts\", concept_list)\n",
    "    concept_template_list = _read_json_attr(f, \"prompt_temp_for_concepts\", concept_prompt_template_list)\n",
    "    label_texts_in_file = _read_json_attr(f, \"label_texts\", label_list)\n",
    "    label_template_list = _read_json_attr(f, \"prompt_temp_for_labels\", label_prompt_template_list)\n",
    "\n",
    "    template_texts_map = {}\n",
    "    if isinstance(concept_template_list, (list, tuple)) and concept_texts_in_file:\n",
    "        for idx, template in enumerate(concept_template_list, start=1):\n",
    "            formatted_texts = [template.format(text) for text in concept_texts_in_file]\n",
    "            template_key = f\"concept_prompts_t{idx:02d}\"\n",
    "            template_texts_map[template_key] = formatted_texts\n",
    "    if isinstance(label_template_list, (list, tuple)) and label_texts_in_file:\n",
    "        for idx, template in enumerate(label_template_list, start=1):\n",
    "            formatted_label_texts = [template.format(text) for text in label_texts_in_file]\n",
    "            template_key = f\"label_prompts_t{idx:02d}\"\n",
    "            template_texts_map[template_key] = formatted_label_texts\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for template_id, template_texts in template_texts_map.items():\n",
    "            text_inputs = processor(text=template_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            text_cls, text_tokens = model.encode_text(text_inputs[\"input_ids\"], normalize=True)  # [K, D], [K, T_txt, D]\n",
    "            text_tokens_proj = model.text_proj(text_tokens) if hasattr(model, \"text_proj\") else text_tokens\n",
    "            write_template(f, template_id=template_id, texts=template_texts, text_features=text_cls, text_token_features=text_tokens_proj)\n",
    "            stored_templates.append((template_id, text_cls.shape[0]))\n",
    "    params_kwargs = {\n",
    "        \"logit_scale\": _maybe_to_scalar(getattr(model, \"logit_scale\", None)),\n",
    "        \"logit_bias\": _maybe_to_scalar(getattr(model, \"logit_bias\", None)),\n",
    "        \"concept_logit_scale\": _maybe_to_scalar(getattr(model, \"concept_logit_scale\", None)),\n",
    "        \"concept_logit_bias\": _maybe_to_scalar(getattr(model, \"concept_logit_bias\", None)),\n",
    "    }\n",
    "    save_model_params(f, **{k: v for k, v in params_kwargs.items() if v is not None})\n",
    "\n",
    "print(\"Stored templates:\")\n",
    "for template_id, count in stored_templates:\n",
    "    print(f\"  - {template_id}: {count} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b7608",
   "metadata": {},
   "source": [
    "## (3) Compute and save image (token) features to H5 (using the Image Encoder)\n",
    "Token refers to each 27×27 patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd931cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip train: already stored 11959/11959 samples.\n",
      "Skip val: already stored 1712/1712 samples.\n",
      "Skip test: already stored 3421/3421 samples.\n",
      "No new samples were appended.\n"
     ]
    }
   ],
   "source": [
    "# Use Image Encoder to compute and save image (token) features to H5\n",
    "\n",
    "# Define data splits and their loaders\n",
    "split_loaders = [\n",
    "    (\"train\", train_loader),\n",
    "    (\"val\", valid_loader),\n",
    "    (\"test\", test_loader),\n",
    "]\n",
    "\n",
    "def encode_and_store_image_features(split_loaders):\n",
    "    \"\"\"Traverse each data split and write image features to HDF5.\"\"\"\n",
    "    written_counts = {}\n",
    "    with torch.no_grad():\n",
    "        with init_file(OUT_PATH, metadata=DB_METADATA) as f:\n",
    "            existing_counts = {}\n",
    "            if \"split_counts\" in f.attrs:\n",
    "                try:\n",
    "                    existing_counts = json.loads(f.attrs[\"split_counts\"])\n",
    "                except Exception:\n",
    "                    existing_counts = {}\n",
    "            start_idx = f[\"image_features\"].shape[0]\n",
    "            for split_name, loader in split_loaders:\n",
    "                expected = len(loader.dataset)\n",
    "                recorded = int(existing_counts.get(split_name, 0) or 0)\n",
    "                if recorded >= expected:\n",
    "                    print(f\"Skip {split_name}: already stored {recorded}/{expected} samples.\")\n",
    "                    continue\n",
    "\n",
    "                processed_in_split = 0\n",
    "                for imgs, labels in tqdm(loader, desc=f\"{split_name} split\"):\n",
    "                    batch_total = imgs.shape[0]\n",
    "                    if processed_in_split + batch_total <= recorded:\n",
    "                        processed_in_split += batch_total\n",
    "                        continue\n",
    "                    if processed_in_split < recorded:\n",
    "                        offset = recorded - processed_in_split\n",
    "                        imgs = imgs[offset:]\n",
    "                        labels = labels[offset:]\n",
    "                        processed_in_split = recorded\n",
    "                        batch_total = imgs.shape[0]\n",
    "\n",
    "                    pixel_inputs = processor(images=imgs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                    pixel_inputs = pixel_inputs[\"pixel_values\"].to(device)\n",
    "                    img_cls, img_tokens = model.encode_image(pixel_inputs, normalize=True)   # [B,1152], [B,729,1152]\n",
    "                    img_tokens_proj = model.image_proj(img_tokens) if hasattr(model, \"image_proj\") else img_tokens  #  [B,729,1152]\n",
    "                    batch_size = img_cls.shape[0]\n",
    "                    batch_ids = np.arange(start_idx, start_idx + batch_size, dtype=np.int64)\n",
    "                    labels_tensor = labels.view(-1) if hasattr(labels, \"view\") else labels\n",
    "                    labels_np = np.asarray(labels_tensor, dtype=np.int64).reshape(-1)\n",
    "                    append_batch(\n",
    "                        f,\n",
    "                        image_feats=img_cls,\n",
    "                        image_token_feats=img_tokens_proj,\n",
    "                        ids=batch_ids,\n",
    "                        split_names=[split_name] * batch_size,\n",
    "                        labels=labels_np,\n",
    "                    )\n",
    "                    start_idx += batch_size\n",
    "                    processed_in_split += batch_size\n",
    "                    written_counts[split_name] = written_counts.get(split_name, 0) + batch_size\n",
    "\n",
    "            # Update split counts in file attributes\n",
    "            for split_name, count in written_counts.items():\n",
    "                existing_counts[split_name] = int(existing_counts.get(split_name, 0) or 0) + count\n",
    "            f.attrs[\"split_counts\"] = json.dumps(existing_counts)\n",
    "            f.flush()\n",
    "\n",
    "    total_new = sum(written_counts.values())\n",
    "    if total_new == 0:\n",
    "        print(\"No new samples were appended.\")\n",
    "    else:\n",
    "        print(f\"Appended {total_new} samples to {OUT_PATH}.\")\n",
    "        for split_name, count in written_counts.items():\n",
    "            print(f\"  - {split_name}: {count}\")\n",
    "\n",
    "encode_and_store_image_features(split_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6e05b",
   "metadata": {},
   "source": [
    "## (4) Validate H5 contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ed83f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples stored: 17092\n",
      "image_features dataset shape: (17092, 1152)\n",
      "image_token_features dataset shape: (17092, 729, 1152)\n",
      "Preview ids: [0 1 2 3 4]\n",
      "Preview labels: [7 3 6 6 7]\n",
      "Preview splits: [b'train' b'train' b'train' b'train' b'train']\n",
      "Recorded split counts: {'train': 11959, 'val': 1712, 'test': 3421}\n",
      "label_texts: ['basophil', 'eosinophil', 'erythroblast', '...']\n",
      "prompt_temp_for_labels: ['a microscopic image of a {} cell', 'a peripheral blood smear image of a {}', 'a bloodcell of {}']\n",
      "concept_texts: ['Segmented nucleus', 'Band nucleus (band form)', 'Reniform / indented nucleus', '...']\n",
      "prompt_temp_for_concepts: ['a cell photo with sign of {}', 'a photo of a cell with {}', 'a cell image indicating {}', '...']\n",
      "Templates stored: ['concept_prompts_t01', 'concept_prompts_t02', 'concept_prompts_t03', 'concept_prompts_t04', 'concept_prompts_t05', 'concept_prompts_t06', 'concept_prompts_t07', 'concept_prompts_t08', 'concept_prompts_t09', 'label_prompts_t01', 'label_prompts_t02', 'label_prompts_t03']\n",
      "  • Template 'concept_prompts_t01' -> K=15, D=1152, sample texts=[b'a cell photo with sign of Segmented nucleus'\n",
      " b'a cell photo with sign of Band nucleus (band form)'\n",
      " b'a cell photo with sign of Reniform / indented nucleus']\n",
      "  • Template 'concept_prompts_t02' -> K=15, D=1152, sample texts=[b'a photo of a cell with Segmented nucleus'\n",
      " b'a photo of a cell with Band nucleus (band form)'\n",
      " b'a photo of a cell with Reniform / indented nucleus']\n",
      "Stored model params: {'logit_scale': 4.570647716522217, 'logit_bias': -11.16147232055664, 'concept_logit_scale': 4.511484146118164, 'concept_logit_bias': -11.0343017578125}\n"
     ]
    }
   ],
   "source": [
    "def validate_h5(path: str = OUT_PATH, max_templates: int = 2):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File '{path}' does not exist yet.\")\n",
    "        return\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        num_samples = f[\"image_features\"].shape[0]\n",
    "        print(f\"Total samples stored: {num_samples}\")\n",
    "        print(f\"image_features dataset shape: {f['image_features'].shape}\")\n",
    "        print(f\"image_token_features dataset shape: {f['image_token_features'].shape}\")\n",
    "        ids_preview = f[\"ids\"][:5]\n",
    "        labels_preview = f[\"labels\"][:5]\n",
    "        split_preview = f[\"split\"][:5]\n",
    "        print(f\"Preview ids: {ids_preview}\")\n",
    "        print(f\"Preview labels: {labels_preview}\")\n",
    "        print(f\"Preview splits: {split_preview}\")\n",
    "        try:\n",
    "            split_counts = json.loads(f.attrs.get(\"split_counts\", \"{}\"))\n",
    "        except Exception:\n",
    "            split_counts = {}\n",
    "        print(f\"Recorded split counts: {split_counts}\")\n",
    "        metadata_keys = [\n",
    "            \"label_texts\",\n",
    "            \"prompt_temp_for_labels\",\n",
    "            \"concept_texts\",\n",
    "            \"prompt_temp_for_concepts\",\n",
    "        ]\n",
    "        for key in metadata_keys:\n",
    "            value = _read_json_attr(f, key, None)\n",
    "            if value is None:\n",
    "                continue\n",
    "            if isinstance(value, list):\n",
    "                preview = value[:3] + ([\"...\"] if len(value) > 3 else [])\n",
    "                print(f\"{key}: {preview}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        templates = list(f[\"templates\"].keys())\n",
    "        print(f\"Templates stored: {templates}\")\n",
    "        for template_id in templates[:max_templates]:\n",
    "            g = f[\"templates\"][template_id]\n",
    "            texts_sample = g[\"texts\"][:3] if \"texts\" in g else []\n",
    "            print(f\"  • Template '{template_id}' -> K={g.attrs.get('K')}, D={g.attrs.get('D')}, sample texts={texts_sample}\")\n",
    "        params = load_model_params(f)\n",
    "        print(f\"Stored model params: {params}\")\n",
    "\n",
    "validate_h5()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg_monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
