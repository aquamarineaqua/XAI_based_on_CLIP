{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01c5a18",
   "metadata": {},
   "source": [
    "## 1 Read Overall Summary\n",
    "\n",
    "Quickly output number of samples, dataset shapes, template list, and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f651c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features: (17092, 1152)\n",
      "ids preview (first 5): [0 1 2 3 4]\n",
      "split counts: {'train': 11959, 'val': 1712, 'test': 3421}\n",
      "templates: ['concept_prompts_t01', 'concept_prompts_t02', 'concept_prompts_t03', '...']\n",
      "\n",
      "H5 structure:\n",
      "\n",
      "conceptclip_features.h5\n",
      "├── image_features               # [17092, 1152]\n",
      "├── image_token_features         # [17092, 729, 1152]\n",
      "├── ids                          # [17092] int64\n",
      "├── labels                       # [17092] int64\n",
      "├── split                        # [17092]\n",
      "├── attrs: {D, T_img, created_at, version, split_counts, logit_scale, logit_bias, concept_logit_scale, concept_logit_bias, label_texts, prompt_temp_for_labels, concept_texts, prompt_temp_for_concepts}\n",
      "└── templates/\n",
      "    ├── concept_prompts_t01/\n",
      "    │   ├── text_features        # [15, 1152] (float32)\n",
      "    │   ├── text_token_features  # [15, 15, 1152] (float32)\n",
      "    │   ├── texts                # [15] (variable length string)\n",
      "    │   └── attrs: {K, D, T_txt, texts_hash, created_at}}\n",
      "    ├── concept_prompts_t02/\n",
      "    │   ├── text_features        # [15, 1152] (float32)\n",
      "    │   ├── text_token_features  # [15, 15, 1152] (float32)\n",
      "    │   ├── texts                # [15] (variable length string)\n",
      "    │   └── attrs: {K, D, T_txt, texts_hash, created_at}}\n",
      "    ├── concept_prompts_t03/\n",
      "    │   ├── text_features        # [15, 1152] (float32)\n",
      "    │   ├── text_token_features  # [15, 13, 1152] (float32)\n",
      "    │   ├── texts                # [15] (variable length string)\n",
      "    │   └── attrs: {K, D, T_txt, texts_hash, created_at}}\n",
      "    └── ...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import h5py, json\n",
    "\n",
    "def _fmt_shape(shape):\n",
    "    try:\n",
    "        return \"[\" + \", \".join(str(int(x)) for x in shape) + \"]\"\n",
    "    except Exception:\n",
    "        return \"[]\"\n",
    "\n",
    "def _dtype(ds):\n",
    "    try:\n",
    "        return str(ds.dtype)\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "def _is_vlen_str(ds):\n",
    "    try:\n",
    "        dt = ds.dtype\n",
    "        return (hasattr(dt, \"metadata\") and dt.metadata and dt.metadata.get(\"vlen\") is str) or str(dt).startswith(\"|S\") or str(dt) == \"object\"\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _root_attr_keys(f):\n",
    "    preferred = [\n",
    "        \"D\",\"T_img\",\"created_at\",\"version\",\"split_counts\",\"logit_scale\",\"logit_bias\",\"concept_logit_scale\",\"concept_logit_bias\",\n",
    "        \"label_texts\",\"prompt_temp_for_labels\",\"concept_texts\",\"prompt_temp_for_concepts\",\n",
    "    ]\n",
    "    exists = [k for k in preferred if k in f.attrs]\n",
    "    others = sorted([k for k in f.attrs.keys() if k not in exists])\n",
    "    return exists, others\n",
    "\n",
    "def _template_attr_keys(g):\n",
    "    preferred = [\"K\",\"D\",\"T_txt\",\"texts_hash\",\"created_at\"]\n",
    "    exists = [k for k in preferred if k in g.attrs]\n",
    "    others = sorted([k for k in g.attrs.keys() if k not in exists])\n",
    "    return exists, others\n",
    "\n",
    "def _build_h5_tree(path: str, max_templates: int = 3):\n",
    "    lines = []\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        lines.append(\"conceptclip_features.h5\")\n",
    "        # Datasets at root\n",
    "        if \"image_features\" in f:\n",
    "            lines.append(f\"├── image_features               # {_fmt_shape(f['image_features'].shape)}\")\n",
    "        if \"image_token_features\" in f:\n",
    "            lines.append(f\"├── image_token_features         # {_fmt_shape(f['image_token_features'].shape)}\")\n",
    "        if \"ids\" in f:\n",
    "            lines.append(f\"├── ids                          # {_fmt_shape(f['ids'].shape)} int64\")\n",
    "        if \"labels\" in f:\n",
    "            lines.append(f\"├── labels                       # {_fmt_shape(f['labels'].shape)} int64\")\n",
    "        if \"split\" in f:\n",
    "            lines.append(f\"├── split                        # {_fmt_shape(f['split'].shape)}\")\n",
    "        # Root attrs\n",
    "        pref, others = _root_attr_keys(f)\n",
    "        attr_list = pref + ([\"...\"] if others else [])\n",
    "        lines.append(\"├── attrs: {\" + \", \".join(attr_list) + \"}\")\n",
    "        # Templates group\n",
    "        if \"templates\" in f:\n",
    "            lines.append(\"└── templates/\")\n",
    "            tmpl_names = sorted(list(f[\"templates\"].keys()))\n",
    "            show = tmpl_names[:max_templates]\n",
    "            for i, tid in enumerate(show):\n",
    "                g = f[\"templates\"][tid]\n",
    "                is_last_template = (i == len(show) - 1) and (len(tmpl_names) <= max_templates)\n",
    "                # Template group header\n",
    "                lines.append(f\"    {'└──' if is_last_template else '├──'} {tid}/\")\n",
    "                # Inside template group\n",
    "                prefix = \"    \" + (\"    \" if is_last_template else \"│   \")\n",
    "                if \"text_features\" in g:\n",
    "                    tf_shape = _fmt_shape(g[\"text_features\"].shape)\n",
    "                    tf_dtype = _dtype(g[\"text_features\"])\n",
    "                    lines.append(f\"{prefix}├── text_features        # {tf_shape} ({tf_dtype})\")\n",
    "                if \"text_token_features\" in g:\n",
    "                    ttf_shape = _fmt_shape(g[\"text_token_features\"].shape)\n",
    "                    ttf_dtype = _dtype(g[\"text_token_features\"])\n",
    "                    lines.append(f\"{prefix}├── text_token_features  # {ttf_shape} ({ttf_dtype})\")\n",
    "                if \"texts\" in g:\n",
    "                    tx_shape = _fmt_shape(g[\"texts\"].shape)\n",
    "                    vlen = \" (variable length string)\" if _is_vlen_str(g[\"texts\"]) else \"\"\n",
    "                    lines.append(f\"{prefix}├── texts                # {tx_shape}{vlen}\")\n",
    "                pref_t, others_t = _template_attr_keys(g)\n",
    "                attr_list_t = pref_t + ([\"...\"] if others_t else [])\n",
    "                lines.append(f\"{prefix}└── attrs: {{\" + \", \".join(attr_list_t) + \"}}\")\n",
    "            if len(tmpl_names) > max_templates:\n",
    "                # Ellipsis for more templates\n",
    "                lines.append(\"    └── ...\")\n",
    "    return lines\n",
    "\n",
    "def summarize_h5(path: str = \"./conceptclip_features.h5\", max_templates: int = 3):\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        print(\"image_features:\", f[\"image_features\"].shape if \"image_features\" in f else None)\n",
    "        print(\"ids preview (first 5):\", f[\"ids\"][:5] if \"ids\" in f else None)\n",
    "        try:\n",
    "            print(\"split counts:\", json.loads(f.attrs.get(\"split_counts\", \"{}\")))\n",
    "        except Exception:\n",
    "            print(\"split counts:\", {})\n",
    "        templates = list(f[\"templates\"].keys()) if \"templates\" in f else []\n",
    "        print(\"templates:\", templates[:max_templates] + ([\"...\"] if len(templates) > max_templates else []))\n",
    "    print(\"\\nH5 structure:\\n\")\n",
    "    for line in _build_h5_tree(path, max_templates=max_templates):\n",
    "        print(line)\n",
    "\n",
    "summarize_h5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ad5f2",
   "metadata": {},
   "source": [
    "## 2 Clear All Template Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a966b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared 0 template groups: []\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import h5py\n",
    "\n",
    "def clear_all_templates(path: str = \"./conceptclip_features.h5\"):\n",
    "    \"\"\"Remove all template groups under the `templates` subtree in the H5 file.\n",
    "    - If the `templates` group does not exist, no action is performed.\n",
    "    - To keep the structure stable, the group is recreated empty after deletion.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    with h5py.File(path, \"a\") as f:\n",
    "        if \"templates\" not in f:\n",
    "            print(\"No 'templates' group found. Nothing to clear.\")\n",
    "            return\n",
    "        # Count and delete the whole group, then recreate an empty one\n",
    "        tmpl_names = list(f[\"templates\"].keys())\n",
    "        del f[\"templates\"]\n",
    "        f.create_group(\"templates\")\n",
    "        f.flush()\n",
    "        print(f\"Cleared {len(tmpl_names)} template groups: {tmpl_names[:5]}\" + (\" ...\" if len(tmpl_names) > 5 else \"\"))\n",
    "\n",
    "# Execute clearing and inspect structure\n",
    "clear_all_templates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eee3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_h5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbf557",
   "metadata": {},
   "source": [
    "## 3 Retrieve Image Features for a Specific Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07966063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_id: 12000, label: 6, split_name: val, img_feat.shape: (1152,), img_tokens.shape: (729, 1152)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_feature': array([-0.0667  ,  0.02164 ,  0.013214, ...,  0.003517,  0.002188,\n",
       "         0.006638], dtype=float16),\n",
       " 'image_token_feature': array([[ 0.172  ,  1.203  ,  0.811  , ..., -0.637  ,  0.4094 , -1.029  ],\n",
       "        [-0.04007,  0.01625,  1.405  , ..., -0.7    ,  1.091  , -0.5845 ],\n",
       "        [-0.3083 , -0.2852 ,  0.763  , ..., -0.2854 ,  0.68   , -0.807  ],\n",
       "        ...,\n",
       "        [-0.2632 ,  0.3813 ,  1.758  , ...,  0.4058 ,  0.644  , -0.603  ],\n",
       "        [-0.2456 ,  0.596  ,  1.686  , ...,  0.4138 ,  1.04   , -0.6357 ],\n",
       "        [ 0.1019 ,  1.638  ,  0.8374 , ...,  0.03757,  0.4624 ,  0.2737 ]],\n",
       "       dtype=float16),\n",
       " 'id': 12000,\n",
       " 'label': 6,\n",
       " 'split': 'val'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Access sample by its global index in the database\n",
    "def access_sample_by_index(index: int, path: str = \"./conceptclip_features.h5\"):\n",
    "\t\"\"\"Access a sample by global index.\n",
    "\t- index: Global index in range [0, N-1], where N is the total number of samples.\n",
    "\tReturns a dict containing image-level and token-level features plus metadata.\n",
    "\t\"\"\"\n",
    "\twith h5py.File(path, \"r\") as f:\n",
    "\t\timg_feat = f[\"image_features\"][index]\n",
    "\t\timg_tokens = f[\"image_token_features\"][index]\n",
    "\t\tsample_id = int(f[\"ids\"][index])\n",
    "\t\tlabel = int(f[\"labels\"][index])\n",
    "\t\tsplit_name = f[\"split\"][index].decode(\"utf-8\") if hasattr(f[\"split\"], \"dtype\") else f[\"split\"][index]\n",
    "\tprint(f\"sample_id: {sample_id}, label: {label}, split_name: {split_name}, img_feat.shape: {img_feat.shape}, img_tokens.shape: {img_tokens.shape}\")\n",
    "\treturn {\n",
    "\t\t\"image_feature\": img_feat,\n",
    "\t\t\"image_token_feature\": img_tokens,\n",
    "\t\t\"id\": sample_id,\n",
    "\t\t\"label\": label,\n",
    "\t\t\"split\": split_name,\n",
    "\t}\n",
    "\n",
    "# Example: access sample at index 12000\n",
    "access_sample_by_index(12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized batch retrieval utilities\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "\n",
    "def _to_str_array(arr):\n",
    "    return np.array([x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else str(x) for x in arr])\n",
    "\n",
    "def access_batch(split: str = \"all\", idx=None, type: str = \"image\", path: str = \"./conceptclip_features.h5\") -> np.ndarray:\n",
    "    \"\"\"Return batched data as a numpy.ndarray.\n",
    "    Args:\n",
    "        split: One of \"all\" | \"train\" | \"test\" | \"val\" | \"arbitrary_name\". Filters by data split; \"all\" means no filtering. can be an arbitrary name if custom splits are used.\n",
    "        idx:   Optional selection over the filtered subset; may be an int, slice, or a sequence of ints. Applies to indices AFTER split filtering. If None, returns all indices in the split.\n",
    "        type:  One of \"image\" | \"patches\" | \"label\" mapping to image_features / image_token_features / labels.\n",
    "        path:  H5 file path.\n",
    "    Returns:\n",
    "        numpy.ndarray with the requested data subset.\n",
    "    Raises:\n",
    "        FileNotFoundError, ValueError, IndexError, KeyError, TypeError for invalid inputs or missing datasets.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    type = type.lower()\n",
    "    key_map = {\n",
    "        \"image\": \"image_features\",\n",
    "        \"patches\": \"image_token_features\",\n",
    "        \"label\": \"labels\",\n",
    "    }\n",
    "    if type not in key_map:\n",
    "        raise ValueError(f\"Unsupported type '{type}'. Use one of {list(key_map.keys())}.\")\n",
    "    target_key = key_map[type]\n",
    "\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        if target_key not in f:\n",
    "            raise KeyError(f\"Dataset '{target_key}' not found in file.\")\n",
    "        total_n = f[target_key].shape[0]\n",
    "\n",
    "        # Split filtering\n",
    "        split = (split or \"all\")\n",
    "        if split == \"all\":\n",
    "            indices_in_split = np.arange(total_n, dtype=np.int64)\n",
    "        else:\n",
    "            if \"split\" not in f:\n",
    "                raise KeyError(\"Dataset 'split' not found for filtering.\")\n",
    "            split_arr = _to_str_array(f[\"split\"][:])\n",
    "\n",
    "            mask = (split_arr == split)\n",
    "            indices_in_split = np.flatnonzero(mask).astype(np.int64)\n",
    "\n",
    "        # Normalize idx and map to global indices\n",
    "        if idx is None:\n",
    "            selected_global_idx = indices_in_split\n",
    "        elif isinstance(idx, slice):\n",
    "            selected_global_idx = indices_in_split[idx]\n",
    "        elif isinstance(idx, (list, tuple, np.ndarray)):\n",
    "            idx_arr = np.asarray(idx, dtype=np.int64)\n",
    "            if idx_arr.size > 0 and (idx_arr.min() < 0 or idx_arr.max() >= indices_in_split.shape[0]):\n",
    "                raise IndexError(\"Index out of range for the selected split.\")\n",
    "            selected_global_idx = indices_in_split[idx_arr]\n",
    "        elif isinstance(idx, (int, np.integer)):\n",
    "            if idx < 0 or idx >= indices_in_split.shape[0]:\n",
    "                raise IndexError(\"Index out of range for the selected split.\")\n",
    "            selected_global_idx = np.array([indices_in_split[int(idx)]], dtype=np.int64)\n",
    "        else:\n",
    "            raise TypeError(\"idx must be None, int, slice, or a sequence of ints.\")\n",
    "\n",
    "        # Fetch from dataset\n",
    "        ds = f[target_key]\n",
    "        result = ds[selected_global_idx]\n",
    "        result = np.asarray(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c43e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 1152), (4, 729, 1152), (3421,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples:\n",
    "imgs = access_batch(split=\"train\", idx=slice(0, 128), type=\"image\")\n",
    "tokens = access_batch(split=\"val\", idx=[0, 2, 4, 1000], type=\"patches\")\n",
    "labels = access_batch(split=\"test\", idx=None, type=\"label\")\n",
    "imgs.shape, tokens.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ba80f",
   "metadata": {},
   "source": [
    "## 4 Retrieve Text Features for a Specific Prompt Template\n",
    "\n",
    "The following example demonstrates how to read CLS and token features for a given template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "templates: ['concept_prompts_t01', 'concept_prompts_t02', 'concept_prompts_t03', 'concept_prompts_t04', 'concept_prompts_t05', 'concept_prompts_t06', 'concept_prompts_t07', 'concept_prompts_t08', 'concept_prompts_t09', 'label_prompts_t01', 'label_prompts_t02', 'label_prompts_t03']\n",
      "['Segmented nucleus', 'Band nucleus (band form)', 'Reniform / indented nucleus', 'Round nucleus', 'Fine azurophilic granules', 'Eosinophilic granules', 'Basophilic granules', 'Basophilic cytoplasm', 'Cytoplasmic vacuoles', 'High nuclear-to-cytoplasmic ratio', 'Pale cytoplasm', 'Nucleated erythrocyte (erythroblast)', 'Platelet fragments / clumps', 'Stain precipitate (artifact)', 'Overlapping cell clumps (artifact)']\n",
      "['a cell photo with sign of {}', 'a photo of a cell with {}', 'a cell image indicating {}', 'an image of a cell showing {}', 'blood cell with {}', 'a blood cell photo with sign of {}', 'a photo of a blood cell with {}', 'a blood cell image indicating {}', 'an image of blood cell showing {}']\n",
      "None\n",
      "4.570647716522217\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Get template names\n",
    "def get_template_names(path: str = \"./conceptclip_features.h5\"):\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        templates = list(f[\"templates\"].keys()) if \"templates\" in f else []\n",
    "    return templates\n",
    "\n",
    "print(\"templates:\", get_template_names())\n",
    "\n",
    "def _read_json_attr(f: h5py.File, key: str, default=None):\n",
    "    \"\"\"Read and parse a JSON-formatted attribute from the H5 file root attrs.\"\"\"\n",
    "    if key not in f.attrs:\n",
    "        return default\n",
    "    raw = f.attrs[key]\n",
    "    if isinstance(raw, (bytes, bytearray)):\n",
    "        raw = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "    if isinstance(raw, str):\n",
    "        try:\n",
    "            return json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            return default\n",
    "    return raw\n",
    "\n",
    "print(_read_json_attr(h5py.File(\"./conceptclip_features.h5\", \"r\"), \"concept_texts\"))\n",
    "print(_read_json_attr(h5py.File(\"./conceptclip_features.h5\", \"r\"), \"prompt_temp_for_concepts\"))\n",
    "print(_read_json_attr(h5py.File(\"./conceptclip_features.h5\", \"r\"), \"labels\"))\n",
    "print(_read_json_attr(h5py.File(\"./conceptclip_features.h5\", \"r\"), \"logit_scale\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import h5py, json\n",
    "\n",
    "def fetch_template_features(template_id=None, *,\n",
    "                            is_concept: bool = True,\n",
    "                            concept_list: list | None = None,\n",
    "                            is_label: bool = False,\n",
    "                            type: str = \"text\",\n",
    "                            path: str = \"./conceptclip_features.h5\"):\n",
    "    \"\"\"Return text/token features for one or multiple templates as numpy arrays.\n",
    "    Args:\n",
    "        template_id: str or iterable of str. When provided, intersect with id set after prefix filtering.\n",
    "        is_concept: Filter templates whose names start with 'concept'. Mutually exclusive with is_label.\n",
    "        concept_list: When is_concept=True, optionally restrict rows to a subset of concept names (must match root attr 'concept_texts'); preserved order.\n",
    "        is_label: Filter templates whose names start with 'label'. Mutually exclusive with is_concept.\n",
    "        type: 'text' -> take 'text_features'; 'tokens' -> take 'text_token_features'.\n",
    "        path: Path to H5 file.\n",
    "    Returns:\n",
    "        If exactly one template selected: numpy.ndarray with shape [K,D] or [K,T_txt,D].\n",
    "        If multiple templates: dict mapping template_id -> numpy.ndarray.\n",
    "    Raises:\n",
    "        ValueError / FileNotFoundError / KeyError / IndexError for invalid arguments or missing data.\n",
    "    \"\"\"\n",
    "    if is_concept and is_label:\n",
    "        raise ValueError(\"is_concept and is_label are mutually exclusive; please set only one True.\")\n",
    "    if not (is_concept or is_label):\n",
    "        # Means: no prefix filtering\n",
    "        pass\n",
    "\n",
    "    sel_type = type.lower().strip()\n",
    "    if sel_type not in {\"text\", \"tokens\"}:\n",
    "        raise ValueError(\"type must be 'text' or 'tokens'\")\n",
    "    ds_key = \"text_features\" if sel_type == \"text\" else \"text_token_features\"\n",
    "\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    # Normalize template_id to a set\n",
    "    if template_id is None:\n",
    "        user_tmpls = None\n",
    "    elif isinstance(template_id, (list, tuple, set, np.ndarray)):\n",
    "        user_tmpls = {str(t) for t in template_id}  # a set\n",
    "    else:\n",
    "        user_tmpls = {str(template_id)}\n",
    "\n",
    "    results = {}\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        if \"templates\" not in f:\n",
    "            raise KeyError(\"Group 'templates' not found.\")\n",
    "        all_tmpls = sorted(list(f[\"templates\"].keys()))\n",
    "\n",
    "        # Prefix filtering\n",
    "        filtered = all_tmpls\n",
    "        if is_concept:\n",
    "            filtered = [t for t in filtered if t.startswith(\"concept\")]\n",
    "        elif is_label:\n",
    "            filtered = [t for t in filtered if t.startswith(\"label\")]\n",
    "\n",
    "        # Intersect with user-provided ids\n",
    "        if user_tmpls is not None:\n",
    "            filtered = [t for t in filtered if t in user_tmpls]\n",
    "            if len(filtered) == 0:\n",
    "                raise ValueError(\"Provided template_id has no intersection with filtered templates.\")\n",
    "\n",
    "        # Concept subset indexing\n",
    "        concept_idx = None\n",
    "        if is_concept and concept_list:\n",
    "            base_concepts = _read_json_attr(f, \"concept_texts\", default=None)\n",
    "            if not base_concepts:\n",
    "                raise KeyError(\"Root attr 'concept_texts' missing or empty; cannot subset by concepts.\")\n",
    "            # create a mapping from concept name to its index\n",
    "            pos = {str(name): i for i, name in enumerate(base_concepts)}\n",
    "            miss = [c for c in concept_list if str(c) not in pos]\n",
    "            if miss:\n",
    "                print(f\"Warning: Concepts not found in concept_texts, will ignore: {miss}\")\n",
    "            keep = [pos[str(c)] for c in concept_list if str(c) in pos]\n",
    "            if not keep:\n",
    "                raise ValueError(\"concept_list empty or no matches in concept_texts.\")\n",
    "            concept_idx = np.array(keep, dtype=np.int64)\n",
    "\n",
    "        # Fetch each template dataset\n",
    "        for tid in filtered:\n",
    "            g = f[\"templates\"][tid]\n",
    "            if ds_key not in g:\n",
    "                # For 'tokens' case may legitimately be absent; skip\n",
    "                continue\n",
    "            arr = g[ds_key][:]  # [K,D] or [K,T_txt,D]\n",
    "            if concept_idx is not None:\n",
    "                try:\n",
    "                    arr = arr[concept_idx]\n",
    "                except Exception as e:\n",
    "                    raise IndexError(f\"Failed to slice template {tid} with selected concept indices: {e}\")\n",
    "            results[tid] = np.asarray(arr)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        raise ValueError(\"No template matched criteria or target dataset missing.\")\n",
    "    if len(results) == 1:\n",
    "        return next(iter(results.values()))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1152)\n"
     ]
    }
   ],
   "source": [
    "# Usage examples:\n",
    "# 1) Return CLS features of all concept templates (restricted to subset order)\n",
    "feats = fetch_template_features(is_concept=True, concept_list=[\"Segmented nucleus\", \"Round nucleus\"], type=\"text\")\n",
    "# 2) Return token features of two specific concept templates\n",
    "feats_map = fetch_template_features(template_id=[\"concept_prompts_t01\", \"concept_prompts_t02\"], is_concept=True, type=\"tokens\")\n",
    "# 3) Return CLS features of label templates\n",
    "label_feats = fetch_template_features(is_concept=False, is_label=True, type=\"text\")\n",
    "print(label_feats['label_prompts_t03'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg_monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
